{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d061e3-82bb-4f4e-b10b-283968bc900e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "36e1d86b-c202-45fd-9c80-8411998b3d8a",
   "metadata": {},
   "source": [
    "какие теги есть в каждом документе, какие есть во всех"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5293e39-4142-49c2-ba87-8cee7f40b167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'2018': {'\\\\comm', '\\\\ftf', '\\\\ge', '\\\\id', '\\\\mb', '\\\\ps', '\\\\srf'}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags = dict()\n",
    "\n",
    "for fn in os.listdir(r'C:\\Users\\mkuzo\\Desktop\\Корпус кпелле'):\n",
    "    if fn.endswith('.txt'):\n",
    "        with open(fn, 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "            \n",
    "        tags[fn[:4]] = set(re.findall(r'^(?:\\\\.+?)(?=\\t)', text, re.MULTILINE))\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "524359ef-03ac-4d56-bef0-e802264c923f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set.intersection(tags['2014'], tags['2018'], tags['kpel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0db7bf55-7b55-408e-b0a4-27f7246e45d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'\\\\id', '\\\\srf', '\\\\ftf', '\\\\ge', '\\\\ps', '\\\\mb'}\n",
      "2\n",
      "{'2018': {('\\\\id', '\\\\srf', '\\\\comm', '\\\\ftf', '\\\\ge', '\\\\ps', '\\\\mb'), ('\\\\id', '\\\\srf', '\\\\ftf', '\\\\ge', '\\\\ps', '\\\\mb')}}\n"
     ]
    }
   ],
   "source": [
    "tags2 = dict()\n",
    "\n",
    "for fn in os.listdir(r'C:\\Users\\mkuzo\\Desktop\\Корпус кпелле'):\n",
    "    if fn.endswith('.txt'):\n",
    "        with open(fn, 'r', encoding='utf8') as f:\n",
    "            text = f.read()\n",
    "        \n",
    "        text = re.sub(r'(?<=\\n)\\t+', '===========', text)\n",
    "        sents = text.split('===========')\n",
    "        curr_tags = set()\n",
    "        for s in sents:\n",
    "            curr_tags.add(tuple(set(re.findall(r'^(?:\\\\.+?)(?=\\t)', s, re.MULTILINE))))\n",
    "        \n",
    "        tags2[fn[:4]] = curr_tags\n",
    "\n",
    "for k, v in tags2.items():\n",
    "    v_list = [set(_) for _ in v if len(_)>1]\n",
    "    print(set.intersection(*v_list))\n",
    "    print(len(v))\n",
    "    \n",
    "print(tags2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a0435338-350d-409a-bbd3-2f578ded71f1",
   "metadata": {},
   "source": [
    "корпус"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db634c1d-7d95-459f-b0bc-e205cf59eefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc:\n",
    "\n",
    "    def __init__(self, uid, name, filename):\n",
    "        self.uid = uid\n",
    "        self.name = name\n",
    "        self.file = filename\n",
    "\n",
    "class Sentence:\n",
    "\n",
    "    def __init__(self, uid, doc_id, pos, text, translation):\n",
    "        self.uid = uid\n",
    "        self.doc_id = doc_id\n",
    "        self.text = text\n",
    "        self.translation = translation\n",
    "        self.pos = pos\n",
    "\n",
    "\n",
    "class Token:\n",
    "\n",
    "    def __init__(self, uid, sent_id, pos, text, ps):\n",
    "        self.uid = uid\n",
    "        self.sent_id = sent_id\n",
    "        self.text = text\n",
    "        self.ps = ps\n",
    "        self.pos = pos\n",
    "        \n",
    "    def __str__(self):\n",
    "        return str(vars(self))\n",
    "        \n",
    "class TokenToMorph:\n",
    "    \n",
    "    def __init__(self, word_id, morph_id, pos):\n",
    "        self.word_id = word_id\n",
    "        self.morph_id = morph_id\n",
    "        self.pos = pos\n",
    "\n",
    "class Morph:\n",
    "    \n",
    "    def __init__(self, uid, text, ge, ps):\n",
    "        self.uid = uid\n",
    "        self.full_text = text\n",
    "        self.text = text.strip('-').strip('_')\n",
    "        self.ge = ge\n",
    "        self.ps = ps\n",
    "        self.isroot = int('-' not in text)\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.full_text == other.full_text and self.ge == other.ge and self.ps == other.ps \n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash((self.full_text, self.ge, self.ps))\n",
    "\n",
    "    def __str__(self):\n",
    "        return str(vars(self))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c84bd89c-4da4-4f2b-a909-7b2610b7231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sent(s):\n",
    "    parsed = dict()\n",
    "    \n",
    "#    parsed['ftf'] = re.sub('\\t', ' ', re.search(r'(?<=^\\\\ftf).+$', s).group(0))\n",
    "    parsed['id'] = re.search(r'(?<=\\\\id\\t)\\d+', s.lower()).group(0)\n",
    "    parsed['ftf'] = re.sub('\\t', ' ', re.search(r'(?<=^\\\\ftf).+$', s.lower(), re.MULTILINE).group(0))\n",
    "    parsed['s_text'] = re.sub('\\t', ' ', re.search(r'(?<=^\\\\srf).+$', s.lower(), re.MULTILINE).group(0))\n",
    "    \n",
    "    parsed['tokens'] = []\n",
    "    for t in re.search(r'(?<=^\\\\srf).+$', s.lower(), re.MULTILINE).group(0).split('\\t'):\n",
    "        if t != '':\n",
    "            parsed['tokens'].extend(re.findall(r'[^./,…\\\\‘;?%]+|[./,…\\\\‘;?%]+', t))\n",
    "    \n",
    "    parsed['mb'] = [re.sub(r'[./,…\\\\‘;?%]+', '', i) for i in re.search(r'(?<=^\\\\mb).+$', s.lower(), re.MULTILINE).group(0).split('\\t') if i != '']\n",
    "    parsed['ge'] = [i for i in re.search(r'(?<=^\\\\ge).+$', s.lower(), re.MULTILINE).group(0).split('\\t') if i != '']\n",
    "    parsed['ps'] = [i for i in re.search(r'(?<=^\\\\ps).+$', s.lower(), re.MULTILINE).group(0).split('\\t') if i != '']\n",
    "    \n",
    "    parsed['mb'] = [i for i in parsed['mb'] if i != '']\n",
    "    \n",
    "    return parsed\n",
    "\n",
    "def find_morph_uid(m, morphs):\n",
    "    for i in morphs:\n",
    "        if m == i:\n",
    "            return i.uid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6de8a2f-f296-441e-866d-c893448a956d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2018_FPhL_voyage_glossed+surface+tones_ gl MKuz.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "        \n",
    "text = re.sub(r'(?<=\\n)\\t+', '===========', text)\n",
    "sents = text.split('===========')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e39a222b-37e7-4047-9231-ec974aae09fc",
   "metadata": {},
   "source": [
    "найти все морфемы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "73c8de35-23e0-475d-b849-d15e03f75ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "morphs = set()\n",
    "\n",
    "for s in sents:\n",
    "    parsed = parse_sent(s)\n",
    "    for i in range(len(parsed['mb'])):\n",
    "        morphs.add(Morph(None, parsed['mb'][i], parsed['ge'][i], parsed['ps'][i]))\n",
    "    \n",
    "\n",
    "curr_morph_uid = 0\n",
    "for m in morphs:\n",
    "    m.uid = curr_morph_uid\n",
    "    curr_morph_uid += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dfac74bb-752a-41e3-b88a-d140f8bef1b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____\n",
      "élɛɛ láa zù hùtíli è kɛ́i kɛ̀â pú 19h lɔ̀wai\n",
      "____\n",
      "mìsílipɛ̀lɛi tíi ɓé kɛ̀â kú yíi mù\n",
      "____\n",
      "nàa ɓé kú kú kée kɔ́lɔŋ nàa depuis ɲèlei tíi kwá kɛ́ kú kée tólui kwá kɛ́ kú kée káa\n"
     ]
    }
   ],
   "source": [
    "curr_doc_id = 0\n",
    "\n",
    "doc = [Doc(curr_doc_id, '2018_FPhL_voyage+NOT_glossed+surface+tones', '2018_FPhL_voyage_glossed+surface+tones_ gl MKuz.txt')]\n",
    "\n",
    "sentences = []\n",
    "tokens = []\n",
    "tok_to_morph = []\n",
    "\n",
    "\n",
    "curr_token_uid = 0\n",
    "\n",
    "\n",
    "for s in sents:\n",
    "    parsed = parse_sent(s)\n",
    "    sentences.append(Sentence(parsed['id'], curr_doc_id, parsed['id'], parsed['s_text'], parsed['ftf']))\n",
    "    \n",
    "    non_punkt_tokens = []\n",
    "    \n",
    "    for i in range(len(parsed['tokens'])):\n",
    "        if not (re.search(r'[./,…\\\\‘;?%]', parsed['tokens'][i]) or parsed['tokens'][i] == '-'):\n",
    "            non_punkt_tokens.append(Token(curr_token_uid, parsed['id'], i, parsed['tokens'][i], None))\n",
    "        \n",
    "        tokens.append(Token(curr_token_uid, parsed['id'], i, parsed['tokens'][i], 'punct'))\n",
    "        \n",
    "        curr_token_uid += 1\n",
    "    \n",
    "    morphs_id_groups = []\n",
    "    prev_morphs = []\n",
    "    for i in range(len(parsed['mb'])):\n",
    "        curr_m = Morph(None, parsed['mb'][i], parsed['ge'][i], parsed['ps'][i])\n",
    "        \n",
    "#        print(*prev_morphs, '===', curr_m)\n",
    "        \n",
    "        if prev_morphs == [] or curr_m.full_text.startswith('-'):\n",
    "            prev_morphs.append(curr_m)\n",
    "        \n",
    "        else:\n",
    "#            morphs_id_groups.append(prev_morphs)\n",
    "            morphs_id_groups.append([find_morph_uid(j, morphs) for j in prev_morphs])\n",
    "            prev_morphs = [curr_m]\n",
    "        \n",
    "    morphs_id_groups.append([find_morph_uid(j, morphs) for j in prev_morphs])\n",
    "            \n",
    "    if len(non_punkt_tokens) != len(morphs_id_groups):\n",
    "        print('____')\n",
    "        print(*[m.text for m in non_punkt_tokens])\n",
    "#        print(*[[m.text for m in x] for x in morphs_id_groups])\n",
    "        continue\n",
    "        \n",
    "    for i in range(len(non_punkt_tokens)):\n",
    "        for j in range(len(morphs_id_groups[i])):\n",
    "            tok_to_morph.append(TokenToMorph(non_punkt_tokens[i].uid, morphs_id_groups[i][j], j))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cc285eb6-f743-49ad-bc9b-45bd6393f5bb",
   "metadata": {},
   "source": [
    "делаем sql таблицу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f541151-f319-40ea-bfad-e45574c14188",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ac0951e-45e4-4fd8-a8a6-286468f696d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables_to_write = {'Docs': doc, 'Sentences': sentences, 'Tokens': tokens, 'TokensToWords': tok_to_morph, 'Morphs': morphs}\n",
    "columns = {'Docs': \"\"\"id INTEGER, name TEXT, filename TEXT \"\"\", \n",
    "           'Sentences': \"\"\" id INTEGER, doc_id INTEGER, pos INTEGER, text TEXT, translation TEXT\"\"\", \n",
    "           'Tokens': \"\"\" id INTEGER, sent_id INTEGER, pos INTEGER, text TEXT, ps TEXT\"\"\", \n",
    "           'TokensToWords': \"\"\" word_id INTEGER, morph_id INTEGER, pos INTEGER \"\"\", \n",
    "           'Morphs': \"\"\" id INTEGER, full_text TEXT, text TEXT, ge TEXT, ps TEXT\"\"\"}\n",
    "\n",
    "\n",
    "queries = {'Docs': [(i.uid, i.name, i.file) for i in doc], \n",
    "           'Sentences': [(i.uid, i.doc_id, i.pos, i.text, i.translation) for i in sentences], \n",
    "           'Tokens': [(i.uid, i.sent_id, i.pos, i.text, i.ps)for i in tokens], \n",
    "           'TokensToWords': [(i.word_id, i.morph_id, i.pos)for i in tok_to_morph], \n",
    "           'Morphs': [(i.uid, i.full_text, i.text, i.ge, i.ps)for i in morphs]}  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30448bc1-c250-4831-a337-7e441bc88456",
   "metadata": {},
   "outputs": [],
   "source": [
    "with sqlite3.connect('kpele_db.db') as db:\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    for k in tables_to_write.keys():\n",
    "        insert_k = []\n",
    "        query = f\"\"\" CREATE TABLE IF NOT EXISTS {k}({columns[k]}) \"\"\"\n",
    "        cursor.execute(query)\n",
    "        \n",
    "    db.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffed877b-e609-4ade-87f2-73fdc4dd9e12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Docs yay\n",
      "Sentences yay\n",
      "Tokens yay\n",
      "TokensToWords yay\n",
      "Morphs yay\n"
     ]
    }
   ],
   "source": [
    "with sqlite3.connect('kpele_db.db') as db:\n",
    "    cursor = db.cursor()\n",
    "    \n",
    "    for k in tables_to_write.keys():\n",
    "        query = f\"\"\" INSERT INTO {k}  VALUES({','.join(['?']*len(queries[k][0]))}); \"\"\"\n",
    "        cursor.executemany(query, queries[k])\n",
    "        print(k, 'yay')\n",
    "    db.commit()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b182ea3d-1c73-4f62-9256-36c93a2b1862",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "##### это для определения использованных символов, это не важно"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e5af0fd6-a2fb-4bdd-b7a2-74b95b03dfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('2018_FPhL_voyage_glossed+surface+tones_ gl MKuz.txt', 'r', encoding='utf8') as f:\n",
    "    text = f.read()\n",
    "        \n",
    "token_strings = set()\n",
    "for i in re.findall(r'^\\\\srf.+$', text, re.MULTILINE):\n",
    "    token_strings.update(i.split('\\t'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eabc9569-864b-47de-aec0-c44120870bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "syms = ''.join(list(token_strings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0b74881e-dbe6-4de9-8471-622090713b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"'\", '̀', 'ɓ', '/', '̂', 'ŋ', 'ɲ', '…', '.', ';', '–', '̧', '\\\\', 'ɛ', '́', '̌', '?', 'ɣ', ',', 'ɔ', '‘', '%', '!', '’'}\n"
     ]
    }
   ],
   "source": [
    "print(set(re.findall(r'[^0-9a-zA-Z-]', syms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "03c9f2f8-c47b-467c-8051-f1986d655ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<callable_iterator at 0x1dc34c2ac80>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.finditer(r'a', 'sd')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
